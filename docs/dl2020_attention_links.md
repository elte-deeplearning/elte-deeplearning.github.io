*[Attention és Transformer olvasnivalók](https://www.notion.so/Attention-s-Transformer-olvasnival-k-5ed15a8d564549ab9ec1028328deab95)*

### Bevezető

[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/) (Elég ezt megnézned)

[https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) (Ha érdekel)

[https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html) (Ez advanced tech, csak az elszántaknak!)

[https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/) (Az OpenAI oldala gyönyörűen meg van csinálva.)

### Fun

[https://talktotransformer.com/](https://talktotransformer.com/)

### Tensorflow tutorial

[https://www.tensorflow.org/tutorials/text/transformer](https://www.tensorflow.org/tutorials/text/transformer) (Csak ha érdekel)

### Cikkek

**Attention Is All You Need**
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
Igen, ez egy 2017-es cikk 7000+ hivatkozással.

**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**
Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
Egy új standard az NLP-ben (natural language processing).